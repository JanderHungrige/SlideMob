{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spellchecker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#spellcheck imports\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspellchecker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpellChecker\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01metree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mElementTree\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mET\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spellchecker'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "#pptx transformation imports\n",
    "import zipfile\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "#spellcheck imports\n",
    "from spellchecker import SpellChecker\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "# translator import\n",
    "import xml.etree.ElementTree as ET\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfrom pptx to xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPTXTransformer:\n",
    "    def __init__(self, extract_path: str):\n",
    "        self.extract_path = extract_path\n",
    "        self.namespaces = None\n",
    "\n",
    "    def extract_pptx(self, pptx_path: str) -> str:\n",
    "        \"\"\"Extract a PPTX file into its XML components.\"\"\"\n",
    "        os.makedirs(self.extract_path, exist_ok=True)\n",
    "        \n",
    "        with zipfile.ZipFile(pptx_path, 'r') as pptx:\n",
    "            pptx.extractall(self.extract_path)\n",
    "        \n",
    "        # Get namespaces right after extraction\n",
    "        self.namespaces = self.get_namespace()\n",
    "        return self.extract_path\n",
    "\n",
    "    def get_namespace(self) -> dict:\n",
    "        \"\"\"Get the namespaces from the first slide XML using text processing.\"\"\"\n",
    "        slide_path = os.path.join(self.extract_path, 'ppt/slides/slide1.xml')\n",
    "        \n",
    "        try:\n",
    "            with open(slide_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                \n",
    "            # Find the root element opening tag\n",
    "            start_idx = content.find('<p:sld')\n",
    "            end_idx = content.find('>', start_idx)\n",
    "            if start_idx == -1 or end_idx == -1:\n",
    "                print(\"Could not find root element\")\n",
    "                return {}\n",
    "            \n",
    "            # Extract the root element declaration\n",
    "            root_declaration = content[start_idx:end_idx]\n",
    "            \n",
    "            # Find all xmlns declarations\n",
    "            namespaces = {}\n",
    "            import re\n",
    "            \n",
    "            # Pattern to match xmlns:prefix=\"uri\" or xmlns=\"uri\"\n",
    "            pattern = r'xmlns(?::([^=]+))?=\"([^\"]+)\"'\n",
    "            matches = re.finditer(pattern, root_declaration)\n",
    "            \n",
    "            for match in matches:\n",
    "                prefix = match.group(1)  # This might be None for default namespace\n",
    "                uri = match.group(2)\n",
    "                if prefix:\n",
    "                    namespaces[prefix] = uri\n",
    "                else:\n",
    "                    namespaces['default'] = uri\n",
    "            \n",
    "            print(\"Extracted namespaces:\", namespaces)\n",
    "            return namespaces\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting namespaces: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def compose_pptx(self, source_path: str, output_pptx: str):\n",
    "        \"\"\"Compose a PPTX file from a directory containing the XML structure.\"\"\"\n",
    "        os.makedirs(os.path.dirname(output_pptx), exist_ok=True)\n",
    "        \n",
    "        with zipfile.ZipFile(output_pptx, 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "            for root, _, files in os.walk(source_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, source_path)\n",
    "                    zf.write(file_path, arcname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate xml - Set Parent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerpointPipeline:\n",
    "    def __init__(self, \n",
    "                 model: str=\"gpt-4\", \n",
    "                 pydentic_model: str=\"gpt-4-turbo-preview\", \n",
    "                 client:str=\"OpenAI\", \n",
    "                 namespaces: dict={'a': 'http://schemas.openxmlformats.org/drawingml/2006/main'}\n",
    "                 ):\n",
    "        #load config file\n",
    "        with open(\"config.json\", \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        self.root_folder = config[\"root_folder\"]\n",
    "        self.pptx_name = config[\"pptx_name\"]\n",
    "        self.openai_api_key = config[\"openai_api_key\"]\n",
    "\n",
    "        self.model = model\n",
    "        self.pydentic_model=pydentic_model\n",
    "        self.client = client\n",
    "        self.namespaces =namespaces \n",
    "\n",
    "        self.pptx_path = os.path.join(self.root_folder, self.pptx_name)\n",
    "        self.extract_path = os.path.join(self.root_folder, 'extracted_pptx')\n",
    "        self.output_folder = os.path.join(self.root_folder, 'translated_pptx')\n",
    "        self.output_pptx_name = f'translated_{self.pptx_name}'\n",
    "        \n",
    "        if client == \"OpenAI\":\n",
    "            self.client = OpenAI(api_key=self.openai_api_key)\n",
    "        else:\n",
    "            print(\"Client not supported (So far only OpenAI is supported)\")\n",
    "\n",
    "        # self.default_namespace =  {\n",
    "        #     'a': 'http://schemas.openxmlformats.org/drawingml/2006/main',\n",
    "        #     'r': 'http://schemas.openxmlformats.org/officeDocument/2006/relationships',\n",
    "        #     'p': 'http://schemas.openxmlformats.org/presentationml/2006/main',\n",
    "        #     'a16': 'http://schemas.microsoft.com/office/drawing/2014/main',\n",
    "        #     'p14': 'http://schemas.microsoft.com/office/powerpoint/2010/main',\n",
    "        #     'mc': 'http://schemas.openxmlformats.org/markup-compatibility/2006',\n",
    "        #     'v': 'urn:schemas-microsoft-com:vml'\n",
    "        # }\n",
    "        \n",
    "    def find_slide_files(self, root_folder: str) -> List[str]:\n",
    "            \"\"\"Find all slide XML files in the folder structure.\"\"\"\n",
    "            slide_files = []\n",
    "            for root, _, files in os.walk(root_folder):\n",
    "                for file in files:\n",
    "                    if file.startswith('slide') and file.endswith('.xml'):\n",
    "                        number_part = file[5:-4]\n",
    "                        if number_part.isdigit():\n",
    "                            slide_files.append(os.path.join(root, file))\n",
    "            return sorted(slide_files)\n",
    "    \n",
    "    def extract_paragraphs(self, xml_file: str) -> List[ET.Element]:\n",
    "        \"\"\"Extract everything inparagraphs from the XML file.\"\"\"\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        return root.findall('.//a:p', self.namespaces)\n",
    "\n",
    "    def extract_text_runs(self, xml_file: str) -> Tuple[List[ET.Element], set]:\n",
    "        \"\"\"Extract text elements that need translation.\"\"\"\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        text_elements = []\n",
    "        original_text_elements = set()\n",
    "  \n",
    "        # Create a backup with the original text elements\n",
    "        for paragraph in root.findall('.//a:p', self.namespaces):\n",
    "            for run in paragraph.findall('.//a:r', self.namespaces):\n",
    "                for original_text_element in run.findall('.//a:t', self.namespaces):\n",
    "                    if original_text_element.text and original_text_element.text.strip():\n",
    "                        original_text_elements.add(original_text_element.text.strip())\n",
    "\n",
    "        # Process paragraphs while preserving structure\n",
    "        for paragraph in root.findall('.//a:p', self.namespaces):\n",
    "            text_parts = []\n",
    "            for text_element in paragraph.findall('.//a:t', self.namespaces):\n",
    "                if text_element.text and text_element.text.strip():\n",
    "                    text_parts.append(text_element.text.strip())\n",
    "            \n",
    "            if text_parts:\n",
    "                text_element = ET.Element('a:t')\n",
    "                text_element.text = ' '.join(text_parts)\n",
    "                text_elements.append(text_element)\n",
    "\n",
    "        print(\"Text elements found:\")\n",
    "        for element in text_elements:\n",
    "            print(f\"- {element.text.strip()}\")     \n",
    "        return text_elements, original_text_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate xml - Spellcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlideSpellChecker(PowerpointPipeline):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.spell = SpellChecker()\n",
    "        # Define namespaces used in PPTX XML\n",
    "        self.namespaces = {\n",
    "            'a': 'http://schemas.openxmlformats.org/drawingml/2006/main',\n",
    "            'p': 'http://schemas.openxmlformats.org/presentationml/2006/main'\n",
    "        }\n",
    "        ET.register_namespace('a', self.namespaces['a'])\n",
    "        ET.register_namespace('p', self.namespaces['p'])\n",
    "\n",
    "    def check_and_fix_slide(self, xml_content):\n",
    "        tree = ET.ElementTree(ET.fromstring(xml_content))\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Find all paragraphs\n",
    "        for paragraph in root.findall('.//a:p', self.namespaces):\n",
    "            self._process_paragraph(paragraph)\n",
    "            \n",
    "        return ET.tostring(root, encoding='unicode')\n",
    "\n",
    "    def _process_paragraph(self, paragraph):\n",
    "        runs = paragraph.findall('a:r', self.namespaces)\n",
    "        i = 0\n",
    "        while i < len(runs):\n",
    "            current_run = runs[i]\n",
    "            \n",
    "            # Check if current run has error attribute\n",
    "            if current_run.get('err') == '1':\n",
    "                # Store original properties\n",
    "                run_props = current_run.find('a:rPr', self.namespaces)\n",
    "                \n",
    "                # Collect text from this and adjacent runs\n",
    "                combined_text = self._collect_adjacent_text(runs, i)\n",
    "                \n",
    "                # Fix spelling and update runs\n",
    "                corrected_text = self._fix_spelling(combined_text)\n",
    "                if corrected_text != combined_text:\n",
    "                    self._update_runs_with_correction(runs, i, corrected_text, run_props)\n",
    "                    \n",
    "                # Merge runs with identical properties\n",
    "                self._merge_identical_runs(paragraph)\n",
    "            \n",
    "            # Check language consistency\n",
    "            self._check_language_consistency(current_run)\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "    def _collect_adjacent_text(self, runs, start_index):\n",
    "        \"\"\"Collects text from adjacent runs that might be part of the same word\"\"\"\n",
    "        text_parts = []\n",
    "        i = start_index\n",
    "        \n",
    "        while i < len(runs):\n",
    "            text_elem = runs[i].find('a:t', self.namespaces)\n",
    "            if text_elem is not None:\n",
    "                text_parts.append(text_elem.text)\n",
    "            i += 1\n",
    "            \n",
    "            # Stop if we hit punctuation or clear word boundary\n",
    "            if text_elem is not None and re.search(r'[.!?,\\s]$', text_elem.text):\n",
    "                break\n",
    "                \n",
    "        return ''.join(text_parts)\n",
    "\n",
    "    def _fix_spelling(self, text):\n",
    "        words = text.split()\n",
    "        corrected_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            if not self.spell.correction(word) == word:\n",
    "                corrected_words.append(self.spell.correction(word))\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "                \n",
    "        return ' '.join(corrected_words)\n",
    "\n",
    "    def _update_runs_with_correction(self, runs, start_index, corrected_text, original_props):\n",
    "        \"\"\"Updates the runs with corrected text while maintaining formatting\"\"\"\n",
    "        # Create new run with corrected text\n",
    "        new_run = ET.Element('a:r')\n",
    "        new_run.append(original_props)\n",
    "        \n",
    "        text_elem = ET.SubElement(new_run, 'a:t')\n",
    "        text_elem.text = corrected_text\n",
    "        \n",
    "        # Replace old runs with new corrected run\n",
    "        parent = runs[start_index].getparent()\n",
    "        parent.remove(runs[start_index])\n",
    "        parent.insert(start_index, new_run)\n",
    "\n",
    "    def _merge_identical_runs(self, paragraph):\n",
    "        \"\"\"Merges adjacent runs with identical properties\"\"\"\n",
    "        runs = paragraph.findall('a:r', self.namespaces)\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(runs) - 1:\n",
    "            current_run = runs[i]\n",
    "            next_run = runs[i + 1]\n",
    "            \n",
    "            if self._runs_have_identical_props(current_run, next_run):\n",
    "                # Merge text content\n",
    "                current_text = current_run.find('a:t', self.namespaces).text\n",
    "                next_text = next_run.find('a:t', self.namespaces).text\n",
    "                current_run.find('a:t', self.namespaces).text = current_text + next_text\n",
    "                \n",
    "                # Remove the merged run\n",
    "                paragraph.remove(next_run)\n",
    "                runs = paragraph.findall('a:r', self.namespaces)\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    def _runs_have_identical_props(self, run1, run2):\n",
    "        \"\"\"Checks if two runs have identical properties\"\"\"\n",
    "        props1 = run1.find('a:rPr', self.namespaces)\n",
    "        props2 = run2.find('a:rPr', self.namespaces)\n",
    "        \n",
    "        if props1 is None or props2 is None:\n",
    "            return False\n",
    "            \n",
    "        # Compare relevant attributes\n",
    "        attrs_to_compare = ['lang', 'sz', 'b', 'i', 'u']\n",
    "        return all(props1.get(attr) == props2.get(attr) for attr in attrs_to_compare)\n",
    "\n",
    "    def _check_language_consistency(self, run):\n",
    "        \"\"\"Checks and fixes language consistency within a run\"\"\"\n",
    "        run_props = run.find('a:rPr', self.namespaces)\n",
    "        if run_props is not None and run_props.get('lang') is None:\n",
    "            # Set default language if missing\n",
    "            run_props.set('lang', 'en-US')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate xml - Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationResponse(BaseModel):\n",
    "    translation: str\n",
    "\n",
    "class SlideTranslator(PowerpointPipeline):\n",
    "    def __init__(self, \n",
    "                 target_language: str,\n",
    "                 Further_StyleInstructions: str = \"None\"): \n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.target_language = target_language\n",
    "\n",
    "        if Further_StyleInstructions != \"None\":\n",
    "            self.Further_StyleInstructions = f\" Here are some further wording style instructions: {self.Further_StyleInstructions}\"\n",
    "        else:\n",
    "            self.Further_StyleInstructions = \"\"\n",
    "\n",
    "    def translate_text(self, text: str) -> str:\n",
    "        \"\"\"Translate text while preserving approximate length and formatting.\"\"\"\n",
    "        prompt = f\"\"\"Translate following this instructions: Maintain similar total character length and preserve any special formatting or technical terms. For the translation do not return any other text than the pure translation.\n",
    "        Translate the text to {self.target_language}.{self.Further_StyleInstructions} Text to translate: {text}\n",
    "        \"\"\"\n",
    "\n",
    "        pydentic_prompt_addition = f\"Respond with a JSON object containing only a 'translation' field with the {self.target_language} translation of this text\"\n",
    "        \n",
    "        if self.model == \"gpt-4\": #non pydentic model\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a professional translator.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0.3,\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Translation error: {e}\")\n",
    "                return text\n",
    "        else: #pydentic model   \n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a professional translator.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt + pydentic_prompt_addition}\n",
    "                    ],\n",
    "                    temperature=0.3,\n",
    "                    response_format={ \"type\": \"json_object\" }\n",
    "                )\n",
    "                translation_response = TranslationResponse.model_validate_json(\n",
    "                    response.choices[0].message.content\n",
    "                )\n",
    "                return translation_response.translation.strip()\n",
    "    \n",
    "            except Exception as e:\n",
    "                if \"Error code: 400\" in str(e):\n",
    "                    print(f\"ERROR We use Pydentic, therefore the model must support json output (e.g. gpt-4-turbo-preview)| Translation error: {e}\")\n",
    "                else:\n",
    "                    print(f\"Translation error: {e}\")    \n",
    "                return text\n",
    "\n",
    "    def create_translation_map(self, text_elements: List[ET.Element], original_text_elements: set) -> dict:\n",
    "        \"\"\"Create a mapping between original text and their translations.\"\"\"\n",
    "        translation_map = {text: \"\" for text in original_text_elements}\n",
    "        \n",
    "        for element in text_elements:\n",
    "            original_text = element.text.strip()\n",
    "            print(f\"LLM fed text: {original_text}\")\n",
    "            if original_text:\n",
    "                translated_text = self.translate_text(original_text)\n",
    "                print(f\"Original paragraph: {original_text}\")\n",
    "                print(f\"Translated paragraph: {translated_text}\\n\")\n",
    "                \n",
    "                prompt = f\"\"\"Match each original text segment with its corresponding part from the translation.\n",
    "                Original segments: {list(original_text_elements)}\n",
    "                Full original text: {original_text}\n",
    "                Full translation: {translated_text}\n",
    "                \n",
    "                Return a JSON object where keys are the original segments and values are their corresponding translations.\n",
    "                Only include segments that appear in the original text.\"\"\"\n",
    "                \n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.pydentic_model,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a professional text alignment expert.\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        temperature=0.3,\n",
    "                        response_format={\"type\": \"json_object\"}\n",
    "                    )\n",
    "                    \n",
    "                    segment_mappings = json.loads(response.choices[0].message.content)\n",
    "                    \n",
    "                    for orig_text, trans_text in segment_mappings.items():\n",
    "                        if orig_text in translation_map:\n",
    "                            translation_map[orig_text] = trans_text\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error matching segments: {e}\")\n",
    "        \n",
    "        print(f\"Translation map: {translation_map}\")\n",
    "        return translation_map\n",
    "\n",
    "    def process_slides(self, folder_path: str):\n",
    "        \"\"\"Main function to process all slides in the presentation.\"\"\"\n",
    "        slide_files = self.find_slide_files(folder_path)\n",
    "        \n",
    "        for slide_file in slide_files:\n",
    "            print(f\"\\nProcessing {os.path.basename(slide_file)}...\")\n",
    "            \n",
    "            # Parse XML while preserving structure\n",
    "            tree = ET.parse(slide_file)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # Extract namespaces from the root element\n",
    "            namespaces = {}\n",
    "            for key, value in root.attrib.items():\n",
    "                if key.startswith('xmlns:'):\n",
    "                    prefix = key.split(':')[1]\n",
    "                    namespaces[prefix] = value\n",
    "            \n",
    "            # Extract and create translation mapping\n",
    "            text_elements, original_text_elements = self.extract_text_runs(slide_file)\n",
    "            translation_map = self.create_translation_map(text_elements, original_text_elements)\n",
    "            \n",
    "            # Update text while preserving XML structure and whitespace\n",
    "            for original_text, translation in translation_map.items():\n",
    "                for element in root.findall('.//a:t', self.namespaces):\n",
    "                    if element.text and element.text.strip() == original_text:\n",
    "                        # Preserve any leading/trailing whitespace from the original\n",
    "                        leading_space = ''\n",
    "                        trailing_space = ''\n",
    "                        if element.text.startswith(' '):\n",
    "                            leading_space = ' '\n",
    "                        if element.text.endswith(' '):\n",
    "                            trailing_space = ' '\n",
    "                        element.text = leading_space + translation.strip() + trailing_space\n",
    "\n",
    "            # Register extracted namespaces\n",
    "            for prefix, uri in namespaces.items():\n",
    "                ET.register_namespace(prefix, uri)\n",
    "            \n",
    "            # Register our known namespaces\n",
    "            for prefix, uri in self.namespaces.items():\n",
    "                ET.register_namespace(prefix, uri)\n",
    "            \n",
    "            # Write back XML while preserving declaration and namespaces\n",
    "            with open(slide_file, 'wb') as f:\n",
    "                tree.write(f, encoding='UTF-8', xml_declaration=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline - Ppt to xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XMLcreator(PowerpointPipeline):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize transformer and translator\n",
    "        self.transformer = PPTXTransformer(self.extract_path)\n",
    "\n",
    "    def extract_pptx(self):\n",
    "        \"\"\"Main method to handle the full translation process\"\"\"\n",
    "        try:\n",
    "            # Extract PPTX\n",
    "            self.transformer.extract_pptx(self.pptx_path)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error translating presentation: {e}\")\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipieline - Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerPointTranslator(PowerpointPipeline):\n",
    "    def __init__(self, target_language:str, Further_StyleInstructions:str=\"None\", Further_SpellCheckInstructions:str=\"None\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize transformer and translator\n",
    "        self.transformer = PPTXTransformer(self.extract_path)\n",
    "        self.spellchecker = SlideSpellChecker(Further_SpellCheckInstructions)\n",
    "        self.translator = SlideTranslator(target_language, Further_StyleInstructions)\n",
    "\n",
    "    def translate_presentation(self):\n",
    "        \"\"\"Main method to handle the full translation process\"\"\"\n",
    "        try:\n",
    "            # Extract PPTX\n",
    "            self.transformer.extract_pptx(self.pptx_path)\n",
    "            \n",
    "            #Get namespaces\n",
    "            namespaces = self.transformer.get_namespace()\n",
    "            self.translator.namespaces = namespaces\n",
    "            \n",
    "            # Process slides\n",
    "            self.translator.process_slides(self.extract_path)\n",
    "            \n",
    "            # Compose final PPTX\n",
    "            output_path = os.path.join(self.output_folder, self.output_pptx_name)\n",
    "            self.transformer.compose_pptx(self.extract_path, output_path)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error translating presentation: {e}\")\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted namespaces: {'a': 'http://schemas.openxmlformats.org/drawingml/2006/main', 'r': 'http://schemas.openxmlformats.org/officeDocument/2006/relationships', 'p': 'http://schemas.openxmlformats.org/presentationml/2006/main'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    root_folder = \"/Users/jwh/Code/Translator\"\n",
    "    pptx_name = \"2024-10-23_ASML_Regulation_and_Governance_GenAI.pptx\"\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    #write root folder, pptx, and openai api key into a config file\n",
    "    with open(\"config.json\", \"w\") as f:\n",
    "        json.dump({\"root_folder\": root_folder, \"pptx_name\": pptx_name, \"openai_api_key\": openai_api_key}, f)\n",
    "\n",
    "    #Initial the ParentClass\n",
    "    PowerpointPipeline_instance = PowerpointPipeline()\n",
    "\n",
    "    \"\"\"\n",
    "    # --- Initial the functionality you want ---\n",
    "    \"\"\"\n",
    "\n",
    "extract = True\n",
    "translate = False\n",
    "\n",
    " # -- Extract PPTX --\n",
    "if extract:\n",
    "    XMLcreator_instance = XMLcreator()\n",
    "    success = XMLcreator_instance.extract_pptx()\n",
    "    \n",
    " # -- Translate PPTX --\n",
    "if translate:\n",
    "    translator = PowerPointTranslator(\n",
    "        target_language=\"German\",\n",
    "        Further_StyleInstructions=\"None\"\n",
    "    )\n",
    "    success = translator.translate_presentation()\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    print(f\"Translation completed successfully. File saved to: {translator.output_folder}/{translator.output_pptx_name}\") if success else print(\"Translation failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stdui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
