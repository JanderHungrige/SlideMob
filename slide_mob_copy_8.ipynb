{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "#pptx transformation imports\n",
    "import zipfile\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "# translator import\n",
    "import xml.etree.ElementTree as ET\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfrom pptx to xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPTXTransformer:\n",
    "    def __init__(self, extract_path: str):\n",
    "        self.extract_path = extract_path\n",
    "        self.namespaces = None\n",
    "\n",
    "    def extract_pptx(self, pptx_path: str) -> str:\n",
    "        \"\"\"Extract a PPTX file into its XML components.\"\"\"\n",
    "        os.makedirs(self.extract_path, exist_ok=True)\n",
    "        \n",
    "        with zipfile.ZipFile(pptx_path, 'r') as pptx:\n",
    "            pptx.extractall(self.extract_path)\n",
    "        \n",
    "        # Get namespaces right after extraction\n",
    "        self.namespaces = self.get_namespace()\n",
    "        return self.extract_path\n",
    "\n",
    "    def get_namespace(self) -> dict:\n",
    "        \"\"\"Get the namespaces from the first slide XML using text processing.\"\"\"\n",
    "        slide_path = os.path.join(self.extract_path, 'ppt/slides/slide1.xml')\n",
    "        \n",
    "        try:\n",
    "            with open(slide_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                \n",
    "            # Find the root element opening tag\n",
    "            start_idx = content.find('<p:sld')\n",
    "            end_idx = content.find('>', start_idx)\n",
    "            if start_idx == -1 or end_idx == -1:\n",
    "                print(\"Could not find root element\")\n",
    "                return {}\n",
    "            \n",
    "            # Extract the root element declaration\n",
    "            root_declaration = content[start_idx:end_idx]\n",
    "            \n",
    "            # Find all xmlns declarations\n",
    "            namespaces = {}\n",
    "            import re\n",
    "            \n",
    "            # Pattern to match xmlns:prefix=\"uri\" or xmlns=\"uri\"\n",
    "            pattern = r'xmlns(?::([^=]+))?=\"([^\"]+)\"'\n",
    "            matches = re.finditer(pattern, root_declaration)\n",
    "            \n",
    "            for match in matches:\n",
    "                prefix = match.group(1)  # This might be None for default namespace\n",
    "                uri = match.group(2)\n",
    "                if prefix:\n",
    "                    namespaces[prefix] = uri\n",
    "                else:\n",
    "                    namespaces['default'] = uri\n",
    "            \n",
    "            print(\"Extracted namespaces:\", namespaces)\n",
    "            return namespaces\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting namespaces: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def compose_pptx(self, source_path: str, output_pptx: str):\n",
    "        \"\"\"Compose a PPTX file from a directory containing the XML structure.\"\"\"\n",
    "        os.makedirs(os.path.dirname(output_pptx), exist_ok=True)\n",
    "        \n",
    "        with zipfile.ZipFile(output_pptx, 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "            for root, _, files in os.walk(source_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, source_path)\n",
    "                    zf.write(file_path, arcname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate xml - Set Parent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerpointPipeline:\n",
    "    def __init__(self, \n",
    "                 api_key: str=\"here goes your api key\", \n",
    "                 model: str=\"gpt-4\", \n",
    "                 pydentic_model: str=\"gpt-4-turbo-preview\", \n",
    "                 client:str=\"OpenAI\", \n",
    "                 namespaces: dict={'a': 'http://schemas.openxmlformats.org/drawingml/2006/main'},\n",
    "                 root_folder: str=\"SlideMob/pptx_goes_here\",\n",
    "                 pptx_name: str=\"presentation1.pptx\"):\n",
    "        \n",
    "        self.model = model\n",
    "        self.pydentic_model=pydentic_model\n",
    "        self.client = client\n",
    "        self.namespaces =namespaces \n",
    "\n",
    "        self.root_folder = root_folder\n",
    "        self.pptx_path = os.path.join(root_folder, pptx_name)\n",
    "        self.extract_path = os.path.join(root_folder, 'extracted_pptx')\n",
    "        self.output_folder = os.path.join(root_folder, 'translated_pptx')\n",
    "        self.output_pptx_name = f'translated_{pptx_name}'\n",
    "        \n",
    "        if client == \"OpenAI\":\n",
    "            self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "        # self.default_namespace =  {\n",
    "        #     'a': 'http://schemas.openxmlformats.org/drawingml/2006/main',\n",
    "        #     'r': 'http://schemas.openxmlformats.org/officeDocument/2006/relationships',\n",
    "        #     'p': 'http://schemas.openxmlformats.org/presentationml/2006/main',\n",
    "        #     'a16': 'http://schemas.microsoft.com/office/drawing/2014/main',\n",
    "        #     'p14': 'http://schemas.microsoft.com/office/powerpoint/2010/main',\n",
    "        #     'mc': 'http://schemas.openxmlformats.org/markup-compatibility/2006',\n",
    "        #     'v': 'urn:schemas-microsoft-com:vml'\n",
    "        # }\n",
    "        \n",
    "    def find_slide_files(self, root_folder: str) -> List[str]:\n",
    "            \"\"\"Find all slide XML files in the folder structure.\"\"\"\n",
    "            slide_files = []\n",
    "            for root, _, files in os.walk(root_folder):\n",
    "                for file in files:\n",
    "                    if file.startswith('slide') and file.endswith('.xml'):\n",
    "                        number_part = file[5:-4]\n",
    "                        if number_part.isdigit():\n",
    "                            slide_files.append(os.path.join(root, file))\n",
    "            return sorted(slide_files)\n",
    "\n",
    "    def extract_text_runs(self, xml_file: str) -> Tuple[List[ET.Element], set]:\n",
    "        \"\"\"Extract text elements that need translation.\"\"\"\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        text_elements = []\n",
    "        original_text_elements = set()\n",
    "  \n",
    "        # Create a backup with the original text elements\n",
    "        for paragraph in root.findall('.//a:p', self.namespaces):\n",
    "            for run in paragraph.findall('.//a:r', self.namespaces):\n",
    "                for original_text_element in run.findall('.//a:t', self.namespaces):\n",
    "                    if original_text_element.text and original_text_element.text.strip():\n",
    "                        original_text_elements.add(original_text_element.text.strip())\n",
    "\n",
    "        # Process paragraphs while preserving structure\n",
    "        for paragraph in root.findall('.//a:p', self.namespaces):\n",
    "            text_parts = []\n",
    "            for text_element in paragraph.findall('.//a:t', self.namespaces):\n",
    "                if text_element.text and text_element.text.strip():\n",
    "                    text_parts.append(text_element.text.strip())\n",
    "            \n",
    "            if text_parts:\n",
    "                text_element = ET.Element('a:t')\n",
    "                text_element.text = ' '.join(text_parts)\n",
    "                text_elements.append(text_element)\n",
    "\n",
    "        print(\"Text elements found:\")\n",
    "        for element in text_elements:\n",
    "            print(f\"- {element.text.strip()}\")     \n",
    "        return text_elements, original_text_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate xml - Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate xml - Spellcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellCheckResponse(BaseModel):\n",
    "    spellcheck: str\n",
    "\n",
    "class SlideSpellChecker(PowerpointPipeline):\n",
    "    def __init__(self, Further_SpellCheckInstructions:str):\n",
    "        super().__init__()\n",
    "\n",
    "        if Further_SpellCheckInstructions != \"None\":\n",
    "            self.Further_SpellCheckInstructions = f\" Here are some further wording style instructions: {self.Further_SpellCheckInstructions}\"\n",
    "        else:\n",
    "            self.Further_SpellCheckInstructions = \"\"\n",
    "\n",
    "        # self.Utilities_instance = Utilities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate xml - Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationResponse(BaseModel):\n",
    "    translation: str\n",
    "\n",
    "class SlideTranslator(PowerpointPipeline):\n",
    "    def __init__(self, target_language:str, Further_StyleInstructions:str=\"None\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.target_language = target_language\n",
    "\n",
    "        if Further_StyleInstructions != \"None\":\n",
    "            self.Further_StyleInstructions = f\" Here are some further wording style instructions: {self.Further_StyleInstructions}\"\n",
    "        else:\n",
    "            self.Further_StyleInstructions = \"\"\n",
    "\n",
    "    def translate_text(self, text: str) -> str:\n",
    "        \"\"\"Translate text while preserving approximate length and formatting.\"\"\"\n",
    "        prompt = f\"\"\"Translate following this instructions: Maintain similar total character length and preserve any special formatting or technical terms. For the translation do not return any other text than the pure translation.\n",
    "        Translate the text to {self.target_language}.{self.Further_StyleInstructions} Text to translate: {text}\n",
    "        \"\"\"\n",
    "\n",
    "        pydentic_prompt_addition = f\"Respond with a JSON object containing only a 'translation' field with the {self.target_language} translation of this text\"\n",
    "        \n",
    "        if self.model == \"gpt-4\": #non pydentic model\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a professional translator.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0.3,\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Translation error: {e}\")\n",
    "                return text\n",
    "        else: #pydentic model   \n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a professional translator.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt + pydentic_prompt_addition}\n",
    "                    ],\n",
    "                    temperature=0.3,\n",
    "                    response_format={ \"type\": \"json_object\" }\n",
    "                )\n",
    "                translation_response = TranslationResponse.model_validate_json(\n",
    "                    response.choices[0].message.content\n",
    "                )\n",
    "                return translation_response.translation.strip()\n",
    "    \n",
    "            except Exception as e:\n",
    "                if \"Error code: 400\" in str(e):\n",
    "                    print(f\"ERROR We use Pydentic, therefore the model must support json output (e.g. gpt-4-turbo-preview)| Translation error: {e}\")\n",
    "                else:\n",
    "                    print(f\"Translation error: {e}\")    \n",
    "                return text\n",
    "\n",
    "    def create_translation_map(self, text_elements: List[ET.Element], original_text_elements: set) -> dict:\n",
    "        \"\"\"Create a mapping between original text and their translations.\"\"\"\n",
    "        translation_map = {text: \"\" for text in original_text_elements}\n",
    "        \n",
    "        for element in text_elements:\n",
    "            original_text = element.text.strip()\n",
    "            print(f\"LLM fed text: {original_text}\")\n",
    "            if original_text:\n",
    "                translated_text = self.translate_text(original_text)\n",
    "                print(f\"Original paragraph: {original_text}\")\n",
    "                print(f\"Translated paragraph: {translated_text}\\n\")\n",
    "                \n",
    "                prompt = f\"\"\"Match each original text segment with its corresponding part from the translation.\n",
    "                Original segments: {list(original_text_elements)}\n",
    "                Full original text: {original_text}\n",
    "                Full translation: {translated_text}\n",
    "                \n",
    "                Return a JSON object where keys are the original segments and values are their corresponding translations.\n",
    "                Only include segments that appear in the original text.\"\"\"\n",
    "                \n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.pydentic_model,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a professional text alignment expert.\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        temperature=0.3,\n",
    "                        response_format={\"type\": \"json_object\"}\n",
    "                    )\n",
    "                    \n",
    "                    segment_mappings = json.loads(response.choices[0].message.content)\n",
    "                    \n",
    "                    for orig_text, trans_text in segment_mappings.items():\n",
    "                        if orig_text in translation_map:\n",
    "                            translation_map[orig_text] = trans_text\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error matching segments: {e}\")\n",
    "        \n",
    "        print(f\"Translation map: {translation_map}\")\n",
    "        return translation_map\n",
    "\n",
    "    def process_slides(self, folder_path: str):\n",
    "        \"\"\"Main function to process all slides in the presentation.\"\"\"\n",
    "        slide_files = self.find_slide_files(folder_path)\n",
    "        \n",
    "        for slide_file in slide_files:\n",
    "            print(f\"\\nProcessing {os.path.basename(slide_file)}...\")\n",
    "            \n",
    "            # Parse XML while preserving structure\n",
    "            tree = ET.parse(slide_file)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # Extract namespaces from the root element\n",
    "            namespaces = {}\n",
    "            for key, value in root.attrib.items():\n",
    "                if key.startswith('xmlns:'):\n",
    "                    prefix = key.split(':')[1]\n",
    "                    namespaces[prefix] = value\n",
    "            \n",
    "            # Extract and create translation mapping\n",
    "            text_elements, original_text_elements = self.extract_text_runs(slide_file)\n",
    "            translation_map = self.create_translation_map(text_elements, original_text_elements)\n",
    "            \n",
    "            # Update text while preserving XML structure and whitespace\n",
    "            for original_text, translation in translation_map.items():\n",
    "                for element in root.findall('.//a:t', self.namespaces):\n",
    "                    if element.text and element.text.strip() == original_text:\n",
    "                        # Preserve any leading/trailing whitespace from the original\n",
    "                        leading_space = ''\n",
    "                        trailing_space = ''\n",
    "                        if element.text.startswith(' '):\n",
    "                            leading_space = ' '\n",
    "                        if element.text.endswith(' '):\n",
    "                            trailing_space = ' '\n",
    "                        element.text = leading_space + translation.strip() + trailing_space\n",
    "\n",
    "            # Register extracted namespaces\n",
    "            for prefix, uri in namespaces.items():\n",
    "                ET.register_namespace(prefix, uri)\n",
    "            \n",
    "            # Register our known namespaces\n",
    "            for prefix, uri in self.namespaces.items():\n",
    "                ET.register_namespace(prefix, uri)\n",
    "            \n",
    "            # Write back XML while preserving declaration and namespaces\n",
    "            with open(slide_file, 'wb') as f:\n",
    "                tree.write(f, encoding='UTF-8', xml_declaration=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerPointTranslator(PowerpointPipeline):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize transformer and translator\n",
    "        self.transformer = PPTXTransformer(self.extract_path)\n",
    "        self.translator = SlideTranslator(target_language, Further_StyleInstructions)\n",
    "\n",
    "    def translate_presentation(self):\n",
    "        \"\"\"Main method to handle the full translation process\"\"\"\n",
    "        try:\n",
    "            # Extract PPTX\n",
    "            self.transformer.extract_pptx(self.pptx_path)\n",
    "            \n",
    "            #Get namespaces\n",
    "            namespaces = self.transformer.get_namespace()\n",
    "            self.translator.namespaces = namespaces\n",
    "            \n",
    "            # Process slides\n",
    "            self.translator.process_slides(self.extract_path)\n",
    "            \n",
    "            # Compose final PPTX\n",
    "            output_path = os.path.join(self.output_folder, self.output_pptx_name)\n",
    "            self.transformer.compose_pptx(self.extract_path, output_path)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error translating presentation: {e}\")\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PowerPointTranslator.__init__() got an unexpected keyword argument 'target_language'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m openai_api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m PowerpointPipeline_instance \u001b[38;5;241m=\u001b[39m PowerpointPipeline(\n\u001b[1;32m      8\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mopenai_api_key,\n\u001b[1;32m      9\u001b[0m     root_folder\u001b[38;5;241m=\u001b[39mroot_folder,\n\u001b[1;32m     10\u001b[0m     pptx_name\u001b[38;5;241m=\u001b[39mpptx_name,\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m translator \u001b[38;5;241m=\u001b[39m \u001b[43mPowerPointTranslator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGerman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFurther_StyleInstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m success \u001b[38;5;241m=\u001b[39m translator\u001b[38;5;241m.\u001b[39mtranslate_presentation()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslation completed successfully. File saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranslator\u001b[38;5;241m.\u001b[39moutput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranslator\u001b[38;5;241m.\u001b[39moutput_pptx_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m success \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslation failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: PowerPointTranslator.__init__() got an unexpected keyword argument 'target_language'"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    root_folder = \"/Users/jwh/Code/Translator\"\n",
    "    pptx_name = \"2024-10-23_ASML_Regulation_and_Governance_GenAI.pptx\"\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    PowerpointPipeline_instance = PowerpointPipeline(\n",
    "        api_key=openai_api_key,\n",
    "        root_folder=root_folder,\n",
    "        pptx_name=pptx_name,\n",
    "    )\n",
    "    translator = PowerPointTranslator(\n",
    "        target_language=\"German\",\n",
    "        Further_StyleInstructions=\"None\"\n",
    "    )\n",
    "    \n",
    "    success = translator.translate_presentation()\n",
    "    print(f\"Translation completed successfully. File saved to: {translator.output_folder}/{translator.output_pptx_name}\") if success else print(\"Translation failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stdui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
